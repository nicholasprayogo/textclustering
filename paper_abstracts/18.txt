The Unsupervised Learning of Natural Language Structure
Authors
Christopher D Manning
Publication date
2018/11/8
Description
There is precisely one complete language processing system to date: the human brain. Though there is debate on how much built-in bias human learne rs might have, we definitely acquire language in a primarily unsupervised fashio n. On the other hand, computational approaches to language processing are almost excl usively supervised, relying on hand-labeled corpora for training. This reliance is largel y due to unsupervised approaches having repeatedly exhibited discouraging performance. In particular, the problem of learning syntax (grammar) from completely unannotated text has r eceived a great deal of attention for well over a decade, with little in the way of positive results. We argue that previous methods for this task have generally underperformed becaus of the representations they used. Overly complex models are easily distracted by non-sy ntactic correlations (such as topical associations), while overly simple models aren’tr ich enough to capture important first-order properties of language (such as directionality, adjacency, and valence). In this work, we describe several syntactic representation s and associated probabilistic models which are designed to capture the basic character of natural language syntax as directly as possible. First, we examine a nested, distribut ional method which induces bracketed tree structures. Second, we examine a dependency model which induces word-to-word dependency structures. Finally, we demonstrate that these two models perform better in combination than they do alone. With these representations, high-quality analyses can be learned from surprisingly little text, with no labeled exam ples 
