Ronan Collobert, Jason Weston
Description
The field of Natural Language Processing (NLP) aims to convert human language into a formal representation that is easy for computers to manipulate. Current end applications include information extraction, machine translation, summarization, search and human-computer interfaces. While complete semantic understanding is still a far-distant goal, researchers have taken a divide and conquer approach and identified several sub-tasks useful for application development and analysis. These range from the syntactic, such as part-of-speech tagging, chunking and parsing, to the semantic, such as word-sense disambiguation, semantic-role labeling, named entity extraction and anaphora resolution. Currently, most research analyzes those tasks separately. Many systems possess few characteristics that would help develop a unified architecture which would presumably be necessary for deeper semantic tasks. In particular, many systems possess three failings in this regard:(i) they are shallow in the sense that the classifier is often linear,(ii) for good performance with a linear classifier they must incorporate many hand-engineered features specific for the task; and (iii) they cascade features learnt separately from other tasks, thus propagating errors.
We propose instead a completely different approach: we advocate the brain way, where features for a task are implicitly trained in a very deep architecture. We describe a general end-to-end system based on a single convolutional neural network architecture. Given a sentence, without any engineered features, it outputs a host of language processing predictions: part-of-speech tags, chunks, named entity 
